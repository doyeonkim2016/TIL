# Summary01 - Understanding Spark Application Concept

22/10/2023
## Spark Application and SparkSession
A core of every Spark application is the Spark driver program which creates a *SparkSession* object. 
Once you have a *SparkSession*, you can program Spark to perform Spark operations.

## Spark Jobs
Driver converts Spark application into one or more Spark jobs. Then transforms each job into a DAG could be a single or multiple Spark stages.

## Spark Stages
As part of DAG nodes, stages are created based on what operations can be performed serially or in parallel.

## Spark Tasks
Each stage is comprised of Spark tasks, which are then federated across each Spark executor; each task maps to a single core and works on a single partition of data.

## Transformations, Actions, and Lazy Evaluation
Spark operations on distributed data can be classified into two types: transformations and actions. 

Transformations, transform a Spark DataFrame into a new DataFrame without altering the original data, giving it the propery of immutability. 

All transformations are evaluated lazily. Results are not computed immediately, but they are recorded or remembered as lineage.

Lazy evaluation allows Spark to optimise your queries by peeking into your chained transformations, lineage and data immutability provide fault tolerance.
